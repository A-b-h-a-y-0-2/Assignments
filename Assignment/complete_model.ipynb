{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2ecf3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 21:17:11.230821: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-06 21:17:11.263293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746546431.298179 1390992 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746546431.308573 1390992 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746546431.337499 1390992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746546431.337538 1390992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746546431.337541 1390992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746546431.337544 1390992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-06 21:17:11.346350: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/multi-lap-49/Downloads/Biosky/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, glob, math, warnings\n",
    "import numpy as np, pandas as pd, xarray as xr\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4dbb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Constants ---\n",
    "DATA_ROOT = \"dataset\"\n",
    "MONTH_FOLDERS = [\"Jun_Output\", \"Jul_Output\", \"Aug_Output\"]\n",
    "GHI_CSV = \"dataset/Sample Dataset - ML Assignment - Sheet1.csv\"\n",
    "\n",
    "WINDOW = 1\n",
    "SHIFT = 1\n",
    "HORIZON = 1\n",
    "MAX_GHI = 1100.0\n",
    "SAT_SCALE = 1023.0\n",
    "CHANNEL_VARS = [\"IMG_MIR\", \"IMG_SWIR\", \"IMG_TIR1\", \"Sun_Elevation\", \"Sat_Elevation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf05af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1733 - mae: 0.4406\n",
      "Epoch 1: val_loss improved from inf to 0.06234, saving model to final_convLSTM_shift3_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 40ms/step - loss: 0.1725 - mae: 0.4396 - val_loss: 0.0623 - val_mae: 0.2767 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0453 - mae: 0.2263\n",
      "Epoch 2: val_loss improved from 0.06234 to 0.04431, saving model to final_convLSTM_shift3_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0453 - mae: 0.2262 - val_loss: 0.0443 - val_mae: 0.2256 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m117/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0330 - mae: 0.1936\n",
      "Epoch 3: val_loss improved from 0.04431 to 0.03498, saving model to final_convLSTM_shift3_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0330 - mae: 0.1936 - val_loss: 0.0350 - val_mae: 0.2018 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0276 - mae: 0.1817\n",
      "Epoch 4: val_loss improved from 0.03498 to 0.03073, saving model to final_convLSTM_shift3_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0276 - mae: 0.1817 - val_loss: 0.0307 - val_mae: 0.1931 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0257 - mae: 0.1786\n",
      "Epoch 5: val_loss improved from 0.03073 to 0.02896, saving model to final_convLSTM_shift3_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.0257 - mae: 0.1786 - val_loss: 0.0290 - val_mae: 0.1909 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m118/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0252 - mae: 0.1786\n",
      "Epoch 6: val_loss improved from 0.02896 to 0.02825, saving model to final_convLSTM_shift3_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - loss: 0.0253 - mae: 0.1786 - val_loss: 0.0282 - val_mae: 0.1906 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0252 - mae: 0.1791\n",
      "Epoch 7: val_loss improved from 0.02825 to 0.02796, saving model to final_convLSTM_shift3_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - loss: 0.0252 - mae: 0.1791 - val_loss: 0.0280 - val_mae: 0.1906 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0252 - mae: 0.1795\n",
      "Epoch 8: val_loss improved from 0.02796 to 0.02784, saving model to final_convLSTM_shift3_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0252 - mae: 0.1795 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m118/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0252 - mae: 0.1797\n",
      "Epoch 9: val_loss improved from 0.02784 to 0.02780, saving model to final_convLSTM_shift3_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - loss: 0.0252 - mae: 0.1797 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m118/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0252 - mae: 0.1797\n",
      "Epoch 10: val_loss improved from 0.02780 to 0.02778, saving model to final_convLSTM_shift3_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0252 - mae: 0.1797 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 11: val_loss improved from 0.02778 to 0.02777, saving model to final_convLSTM_shift3_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 12: val_loss improved from 0.02777 to 0.02776, saving model to final_convLSTM_shift3_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 13: val_loss improved from 0.02776 to 0.02776, saving model to final_convLSTM_shift3_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0252 - mae: 0.1799\n",
      "Epoch 14: val_loss did not improve from 0.02776\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 0.0252 - mae: 0.1799 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 15: val_loss did not improve from 0.02776\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 16: val_loss did not improve from 0.02776\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m117/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 17: val_loss did not improve from 0.02776\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.02776\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m117/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 19: val_loss did not improve from 0.02776\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 2.5000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 20: val_loss did not improve from 0.02776\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 2.5000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 21: val_loss did not improve from 0.02776\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 2.5000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m118/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 22: val_loss did not improve from 0.02776\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 2.5000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.02776\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 2.5000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m117/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 24: val_loss did not improve from 0.02776\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 1.2500e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0252 - mae: 0.1798\n",
      "Epoch 25: val_loss did not improve from 0.02776\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0252 - mae: 0.1798 - val_loss: 0.0278 - val_mae: 0.1907 - learning_rate: 1.2500e-04\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f1ea0ce39e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Setup ---\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Load GHI CSV ---\n",
    "ghi_df = (pd.read_csv(GHI_CSV)\n",
    "            .rename(columns={\"Date \": \"timestamp\", \"Observed GHI \": \"GHI\"}))\n",
    "ghi_df[\"timestamp\"] = pd.to_datetime(ghi_df[\"timestamp\"])\n",
    "ghi_dict = ghi_df.set_index(\"timestamp\")[\"GHI\"].to_dict()\n",
    "\n",
    "# --- Load .nc sample for training ---\n",
    "def load_day_nc(nc_path, vars_to_use):\n",
    "    ds = xr.open_dataset(nc_path)\n",
    "    daily_stack, ghi_labels = [], []\n",
    "    for t_idx in range(len(ds.time)):\n",
    "        t_stamp = pd.to_datetime(str(ds.time[t_idx].values))\n",
    "        if t_stamp not in ghi_dict:\n",
    "            continue\n",
    "        channels = [ds[var].isel(time=t_idx).values.astype(\"float32\") for var in vars_to_use]\n",
    "        patch = np.stack(channels, axis=-1)\n",
    "        daily_stack.append(patch)\n",
    "        ghi_labels.append(ghi_dict[t_stamp])\n",
    "    if len(daily_stack) >= (WINDOW + SHIFT):\n",
    "        return np.stack(daily_stack), np.array(ghi_labels, dtype=\"float32\")\n",
    "    return None, None\n",
    "\n",
    "# --- Build dataset ---\n",
    "Xs, ys = [], []\n",
    "all_nc_paths = []\n",
    "for month in MONTH_FOLDERS:\n",
    "    all_nc_paths += glob.glob(os.path.join(DATA_ROOT, month, \"**\", \"*.nc\"), recursive=True)\n",
    "\n",
    "for nc_path in all_nc_paths:\n",
    "    X_day, y_day = load_day_nc(nc_path, CHANNEL_VARS)\n",
    "    if X_day is None: continue\n",
    "\n",
    "    ghi_plane = y_day.reshape(-1, 1, 1, 1)\n",
    "    ghi_plane = np.repeat(ghi_plane, 5, axis=1)\n",
    "    ghi_plane = np.repeat(ghi_plane, 5, axis=2)\n",
    "    X_day = np.concatenate([X_day, ghi_plane], axis=-1)\n",
    "\n",
    "    for t0 in range(len(X_day) - WINDOW + 1):\n",
    "        if t0 + WINDOW - SHIFT + HORIZON > len(y_day):\n",
    "            continue\n",
    "        Xs.append(X_day[t0:t0+WINDOW])\n",
    "        ys.append(y_day[t0 + WINDOW - SHIFT : t0 + WINDOW - SHIFT + HORIZON])\n",
    "\n",
    "X = np.stack(Xs).astype(\"float32\") / SAT_SCALE\n",
    "y = np.stack(ys).astype(\"float32\") / MAX_GHI\n",
    "\n",
    "# --- Train/val split ---\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n",
    "\n",
    "# --- Define ConvLSTM model ---\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.ConvLSTM2D(32, (3,3), activation='relu', return_sequences=True,\n",
    "                               input_shape=(WINDOW, 5, 5, X.shape[-1]), padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ConvLSTM2D(16, (1,1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(HORIZON)\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss=tf.keras.losses.Huber(), metrics=[\"mae\"])\n",
    "\n",
    "# --- Callbacks ---\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=12, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"final_convLSTM_shift3_best.h5\", save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# --- Train model ---\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          epochs=100, batch_size=16,\n",
    "          callbacks=callbacks, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e1d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MONTH = \"dataset/Sep_Validation_Data/INSAT_Validation_Data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed5e936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    644.000000\n",
      "mean     338.921173\n",
      "std        0.000000\n",
      "min      338.921173\n",
      "25%      338.921173\n",
      "50%      338.921173\n",
      "75%      338.921173\n",
      "max      338.921173\n",
      "Name: predicted_GHI, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"sept_forecast.csv\")\n",
    "print(df[\"predicted_GHI\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9af1ce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 100%|██████████| 14/14 [00:39<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "forecast_rows = []\n",
    "\n",
    "for nc_path in tqdm(glob.glob(f\"{TEST_MONTH}/**/*.nc\", recursive=True), desc=\"Inferencing\"):\n",
    "    X_day, time_vec = load_day_nc_infer(nc_path, CHANNEL_VARS)\n",
    "    if X_day is None: continue\n",
    "\n",
    "    T = X_day.shape[0]\n",
    "    past_ghi = np.zeros(T, dtype=\"float32\")  # rolling GHI predictions\n",
    "\n",
    "    for end_idx in range(WINDOW - 1, T):\n",
    "        start_idx = end_idx - WINDOW + 1\n",
    "        X_window = X_day[start_idx:end_idx + 1]\n",
    "\n",
    "        # Construct rolling GHI input channel\n",
    "        ghi_window = past_ghi[start_idx:end_idx + 1]\n",
    "        ghi_stack = np.stack([np.full((5, 5), val, dtype=\"float32\") for val in ghi_window])\n",
    "        ghi_stack = ghi_stack[..., None]  # (WINDOW, 5, 5, 1)\n",
    "\n",
    "        # Concatenate channels and normalize\n",
    "        window = np.concatenate([X_window, ghi_stack], axis=-1)[None, ...]  # (1, WINDOW, 5, 5, C+1)\n",
    "        window = window / SAT_SCALE\n",
    "\n",
    "        # Predict\n",
    "        pred_norm = model.predict(window, verbose=0)[0][0]\n",
    "        pred_wm2 = float(pred_norm * MAX_GHI)\n",
    "        pred_wm2 = max(pred_wm2, 0.0)\n",
    "\n",
    "        # Update rolling GHI\n",
    "        past_ghi[end_idx] = pred_wm2\n",
    "\n",
    "        # Record prediction\n",
    "        ts = pd.to_datetime(str(time_vec[end_idx]))\n",
    "        forecast_rows.append({\"timestamp\": ts, \"predicted_GHI\": pred_wm2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7c766be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315 missing slots – linearly interpolating.\n",
      "✅  sept_forecast.csv written – rows: 672\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Convert forecast to DataFrame ---\n",
    "df_pred = pd.DataFrame(forecast_rows)\n",
    "\n",
    "# --- Sort & remove duplicates ---\n",
    "df_pred = (\n",
    "    df_pred\n",
    "    .drop_duplicates(\"timestamp\")\n",
    "    .sort_values(\"timestamp\")\n",
    "    .set_index(\"timestamp\")\n",
    ")\n",
    "\n",
    "# --- Create complete 15-min interval index for Sept 1–7 ---\n",
    "full_index = pd.date_range(\"2024-09-01 00:00\", \"2024-09-07 23:45\", freq=\"15min\")\n",
    "df_pred = df_pred.reindex(full_index)\n",
    "\n",
    "# --- Interpolate any missing timestamps ---\n",
    "missing = df_pred[\"predicted_GHI\"].isna().sum()\n",
    "if missing:\n",
    "    print(f\"{missing} missing slots – linearly interpolating.\")\n",
    "    df_pred[\"predicted_GHI\"] = df_pred[\"predicted_GHI\"].interpolate(method=\"time\")\n",
    "\n",
    "# --- Final clipping (for safety) ---\n",
    "df_pred[\"predicted_GHI\"] = df_pred[\"predicted_GHI\"].clip(lower=0, upper=MAX_GHI)\n",
    "\n",
    "# --- Save to CSV ---\n",
    "df_pred.reset_index().rename(columns={\"index\": \"timestamp\"})\\\n",
    "      .to_csv(\"sept_forecast.csv\", index=False)\n",
    "\n",
    "print(\"✅  sept_forecast.csv written – rows:\", len(df_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b053b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
